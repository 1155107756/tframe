{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "---\n",
    "Assume the network processes as time steps $\\{t~\\rvert~t\\in[1, T]\\}$ and the overall network error at time $t$ is denoted as $E(t)$. Define $E^{total}$ as $\\displaystyle \\sum_t E(t)$.\n",
    "\n",
    "Each weight $w_{lm}$ in the network appears at each time step and is denoted as $w_{lm}(t)$ correspondingly. We have\n",
    "\n",
    "$$\\frac{\\partial E(t)}{\\partial w_{lm}} \n",
    "= \\sum_{\\tau\\le t}\\frac{\\partial E(t)}{\\partial w_{lm}(\\tau)} $$\n",
    "\n",
    "We define $d w_{lm}(t, \\tau)$ as $\\displaystyle\\frac{\\partial E(t)}{\\partial w_{lm}(\\tau)}$ for simplification, thus\n",
    "\n",
    "$$d w_{lm} = \\sum_t d w_{lm}(t)= \\sum_t\\sum_{\\tau\\le t}d w_{lm}(t, \\tau)$$\n",
    "\n",
    "It is the sum of elements in the lower-triangular weight matrix (denoted as $d W_{lm}$):\n",
    "\n",
    "$$\n",
    "d W_{lm} = \\begin{bmatrix}\n",
    "d w_{lm}(1, 1) \\\\\n",
    "d w_{lm}(2, 1) & d w_{lm}(2, 2) \\\\\n",
    "\\vdots & \\vdots & \\ddots \\\\\n",
    "d w_{lm}(T-1, 1) & d w_{lm}(T-1, 2) & \\cdots  & d w_{lm}(T-1, T-1) \\\\\n",
    "d w_{lm}(T, 1) & d w_{lm}(T, 2) & \\cdots & d w_{lm}(T, T-1) & d w_{lm}(T, T)\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The $t^{th}$ row $d W_{lm}(t, :)$ represents the sensitivities of the error $E(t)$ to small perturbations in the weight $w_{lm}$ in previous time steps, i.e. $\\big\\{w_{lm}(\\tau)\\big\\}_{\\tau\\le t}$. The $\\tau^{th}$ column $dW_{lm}(:, \\tau)$ implies how a minor oscillation in $w_{lm}$  can affect the overall performance at the subsequent time steps, i.e. $\\big\\{E(t)\\big\\}_{t\\ge\\tau}$.\n",
    "\n",
    "Note that $\\forall l_1, l_2, m_1, m_2, t_1, t_2, \\tau_1, \\tau_2$, $d w_{l_1~m_1}(t_1, \\tau_1)$ and $d w_{l_2~m_2}(t_2, \\tau_2)$ are independent since weights are assumed to be fixed during one train step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error signals flow down the model spatially, and flow back through each time step temporarily. For each neuron or unit $l$, we define $y_l(t)$ as its output at time $t$ and $dy_l(t)$ as $\\displaystyle \\frac{\\partial E^{total}}{\\partial y_l(t)}$. Since a small perturbation in $y_l(t)$ will not affect the value of network error at previous time steps, we have\n",
    "\n",
    "$$dy_l(t) = \\frac{\\partial \\sum_{\\tau\\ge t}E(\\tau)}{\\partial y_l(t)} = \\sum_{\\tau\\ge t}\\frac{\\partial E(\\tau)}{\\partial y_l(t)}$$\n",
    "\n",
    "Unlike the gradients of weights, elements in $\\big\\{dy_l(t)\\big\\}_{l,t}$ are not independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPTT (Back Propagation Through Time)\n",
    "Methods of backpropagation through time begin at time step $T~$:\n",
    "\n",
    ">For each $w_{lm}$, initialize $dw_{lm}$ as $0$  \n",
    ">**for** $t$ **from** $T$ **downto** $1$ **do**  \n",
    ">$\\quad$For each neuron or unit $l$, calculate the error signal: $dy_l(t) = dy_l^{spatial}(t) + dy_l^{temporal}(t)$  \n",
    ">$\\quad$For each weight $w_{lm}$, calculate $\\displaystyle \\sum_{\\tau\\ge t}dw_{lm}(\\tau, t) = dy_l(t)\\cdot\\frac{\\partial y_l(t)}{\\partial w_{lm}(t)}$ \n",
    "and added it to $dw_{lm}$   \n",
    ">**end for**\n",
    "\n",
    "From the perspective of summing up the elements in $dW_{lm}$, (epochwise) BPTT calculates the sum column by colunm backwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RTRL (Real-Time Recurrent Learning)\n",
    "RTRL accumulates $dw_{lm}$ via \n",
    "$$p_{lm}^{~k}(t)=\\frac{\\partial y_k(t)}{\\partial w_{lm}}$$\n",
    "\n",
    "which can be updated iteratively during forward propagation. Note that there is no time step specification for $w_{lm}$ in the definition above.\n",
    "\n",
    "With this definition, we can easily find that, for vanilla RNN:\n",
    "\n",
    "\\begin{align}\n",
    "p_{lm}^{~k}(t+1) & = f'_k(s_k(t+1))\\cdot\\frac{\\partial s_k(t+1)}{\\partial w_{lm}} \\\\\n",
    "& = f'_k(s_k(t+1))\\cdot \\Big[\\sum_{j\\in U}w_{kj}\\cdot p_{lm}^{~j}(t)+~\\delta(k, l)\\cdot x_m(t+1)\\Big]\n",
    "\\end{align}\n",
    "\n",
    "in whick $p_{lm}^{~k}(0) = 0$, $s_k(t)$ is the input of neuron $k$, $x_m(t)$ is an external input at time $t$ and $U$ is the indices of non-input units.\n",
    "\n",
    "Once $p_{lm}^{~k}(t)$ is obtained, the derivative of $w_{lm}$ can be calculated as\n",
    "\n",
    "$$dw_{lm}=\\sum_t\\frac{\\partial E(t)}{\\partial w_{lm}} = \\sum_t\\frac{\\partial E(t)}{\\partial y_k(t)}\\cdot\\frac{\\partial y_k(t)}{\\partial w_{lm}} = \\sum_te_k(t)\\cdot p_{lm}^{~k}(t)$$\n",
    "\n",
    "In terms of accumulating $dW_{lm}$ elements, RTRL calculates the sum row by row forwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. Ronald J. Williams, David Zipser. Gradient-based learning algorithms for recurrent networks and their computational complexity. Backpropagation, Pages 433-486. 1995.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
